API_KEY=

LLM_URL_SERVER="http://localhost:11434/"
LLM_URL_EMBEDDING_SERVER="http://localhost:11434/"
LLM_MODEL=llama3.2
# LLM_EMBEDDINGMODEL=jina/jina-embeddings-v2-base-de
LLM_EMBEDDINGMODEL=bge-m3

OPENAI_API_KEY=
GROQ_API_KEY=

AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_DEPLOYMENT=
OPENAI_API_VERSION=

# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=
# LANGCHAIN_PROJECT=

LANGFUSE_SECRET_KEY=
LANGFUSE_PUBLIC_KEY=
LANGFUSE_HOST="http://<1.2.3.4>:3010"


TAVILY_API_KEY=

# USE_GROQ=True
# USE_OPENAI=True
# USE_OPENAI_EMBEDDING=True

USE_AZURE=True
USE_AZURE_EMBEDDING=True

OPENAI_CHAT_MODEL="gpt-4o-mini"
GROQ_CHAT_MODEL=llama-3.3-70b-versatile
# llama3-70b-8192

CHUNK_SIZE=1000
CHUNK_OVERLAP=100


USE_PGVECTOR=True
PGVECTOR_USER="assistant"
PGVECTOR_PASSWORD="assistant"
PGVECTOR_HOST="localhost"
PGVECTOR_PORT=5432
PGVECTOR_DB="ki_werkstatt"
PGVECTOR_COLLECTION="robo_chat"


# nur f√ºr Tests im pipelines-Ordner
URL_RAG="http://localhost:8000"
